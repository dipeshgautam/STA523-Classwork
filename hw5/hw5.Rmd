---
title: "Homework 5 - Sampler Potpourri"
author: "Andrew Wong, Dipesh Gautam, Heather Shapiro, Siyang Li"
date: "11/23/2014"
output: html_document
---

## Introduction
The goal of homework 5 was to learn how to implement univariate sampling functions from random probability density functions. The steps to solving this problem included understanding the algorithm involved, coding the samplers, then optimizing them for speed and accuracy.

## Sampling Methodologies
### Rejection Sampler
Our implementation of the rejection sampler first checks that the arguments for each parameter are of the correct type, e.g. sample size, *n* is a positive integer. The coding process first involved writing pseudocode to get the logical framework down. After determining that the pseudocode captured the steps of a rejection sampler, the real coding began. The first iteration of the rejection sampler iteratively sampled proposal values for x and y, before doing the "coin toss" to determine acceptance then stored the accepted values one at a time. Next, multi-core capability was added if the `mc` argument read `TRUE` with a `FALSE` default setting so that using multiple cores has to be a deliberate decision by the user. An additional safeguard for proper usage of multiple cores was a requirement that the number of desired accepted samples had to exceed a certain threshold. The sample size minimum ensured that after ammortization of the multiple core overhead, utilizing multiple cores actually outperformed a single core by minimizing "wall-clock" processing time. 

However, an optimization problem remained. Since rejection sampling performs poorly when a distribution is highly concentrated due to the wasted space (without an adaptive extension), it can require many samples to find the distribution. Using an iterative process magnified this issue. Therefore, it seemed that vectorization was a reasonable expectation to address the large amount of sampling needed to find the desired distribution. A `while` loop was utilized to ensure that the `accepted` values matched or exceeded the amount desired, `n`. Within the loop, two vectors of random values were generated from the instrumental/proposal uniform distribution. The `proposal_x` had a range given by the user for the desired distribution. the `proposal_y` had a range with a max equal to `M`, defined as the maximum `y` value for the desired distribution. `M` would take on the value 100 in the case of the beta distribution, since it goes to infinity at the edges. The amount of samples drawn was chosen to be *1.3 * n*. The assumption was that the acceptance ratio would be around 0.7 for most of the distributions, so an additional 0.3 would be needed to meet the required `n` number of accepted values. `accepted` was subset, `accepted[1:n]`, in case the number of elements in `accepted` was more than `n`.

After completing the vectorization improvement, the next step was to see what broke in the process. After a pleasantly short debugging process, the rejection sampler generated histograms clearly delineating all of the test distributions and scored well on all of them (~0.000) using the provided `score()` function.

### Metropolis-Hastings Sampler
For our implementation of the Metropolis-Hastings Sampler we used the Random normal walk method. First we had to tune the variance of the proposal distribution to get the desired acceptance ratio between 0.25 and 0.4. To accomplish this, we started with a variance of .75 and ran the Metropolis-Hastings algorithm on a smaller sample size. We calculated the acceptance ratio for given variance and if it was not within the desired range, variance was tweaked by either scaling it up or down depending on whether the acceptance ratio was too hign or to low. After getting the desired acceptance ratio, we fixed our variance for the proposal distribution and ran the Metropolis-Hastings algorithm for the given number of iterations.

In MH algorithm, we sample $x^*$ from the proposal distribution, normal with x(sample from previous iteration) as mean and given varaince in case of normal random walk, and accept the $x^*$ as a sample with a probability given by $min\{1, \frac{dfunc(x^*)*g(x^*->x)}{dfunc(x)*g(x->x^*)}\}$. Since we are using symmetric proposal function, our acceptance probability becomes $min\{1, \frac{dfunc(x^*)}{dfunc(x)}\}$

To make the operations faster, when the multi-core option was selected, we split the required number of samples evely between different cores using mcapply function and combined the result back to get required number of samples. We ran multiple cores only if the number of iterations was larger than 1000 as running multiple cores was found to be inefficient for smaller sample sizes.

### Slice Sampler
For slice sampling, we adopted the stepping-out method for finding the correct interval. We step out on both direction with width w from x0 until both ends are just outside the slice. Then, we sample x1 from this range, rejecting x1 if it's sampled outside the slide and reassigning the x1 as the boundary of interval on appropriate side.

We also noticed that using multiple cores was not efficient for lower sample size due to associated overhead in breaking up and combining the data. Due to this fact, we assigned multi-core implementation only when sample-size was greater than 1000.

We noticed that our score for slice sampler was on par with the score for R's built-in samplers and even better than our implementation of R's truncated normal sampling using package "truncnorm".
We were able to get good accuracy accross the board but as expected our implementation turned out to be slower. The speeds improved greatly with the multi core implementation and we were able to get much closer times to R's built-in samplers.


### R Sampler
Implementation of R's built-in samplers was mostly straightforward. We used rbeta(), runif() and rexp() to sample from beta, uniform and exponential distribution respectively. We used rtruncnorm() from the package "truncnorm" to sample truncated normal distributions. We couldn't find a function or package to sample from truncated exponential distribution, so we sampled few extra points from a regular exponential distribution and rejected the ones that were outside the support of the truncated exponential provided.

## Results
We ran the four samplers for the given six distributions and recorded the result in the table below. We have time/samples across all the samplers and distributions for n=100, 10,000, 1,000,000 and 10,000,000. We also record score for each sampler and distribution combination with 1,000,000 samples. All these tests were done both for single core and multi-core implementations of our samplers.The scores are given for the runs with a sample size of 1 million. 
```{r results="asis", echo=FALSE}
source("check_packages.R")
suppressMessages(check_packages(c('xtable')))
table = readRDS("table.Rdata")
table.xtable = xtable(table)
display(table.xtable)= c("d", "s","s", "e", "e","e", "e","e", "e","e", "e","e", "e")
print(table.xtable, type="html")
```




